{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XKPtcEYycuOM",
        "outputId": "3f997c15-7eee-4d3a-8bfe-17074a7f04c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pythainlp\n",
            "  Downloading pythainlp-5.1.2-py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.12/dist-packages (from pythainlp) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31->pythainlp) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31->pythainlp) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31->pythainlp) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31->pythainlp) (2025.8.3)\n",
            "Downloading pythainlp-5.1.2-py3-none-any.whl (19.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.3/19.3 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pythainlp\n",
            "Successfully installed pythainlp-5.1.2\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement difflib (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for difflib\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install pythainlp\n",
        "!pip install difflib\n",
        "!pip install requests\n",
        "!pip install sentence_transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "import difflib\n",
        "import requests\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from pythainlp.tokenize import word_tokenize\n",
        "from pythainlp.corpus import thai_words\n",
        "from pythainlp.tag import pos_tag\n",
        "from pythainlp.util import normalize\n",
        "\n",
        "# ---- ส่วนให้คะแนนใจความ ----\n",
        "\n",
        "# โหลดโมเดล multilingual ---- paraphrase-multilingual-MiniLM-L12-v2 , airesearch/wangchanberta-base-att-spm-uncased\n",
        "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
        "core_sentences = {\n",
        "    \"score_1\": \"สื่อสังคมหรือสื่อออนไลน์หรือสื่อสังคมออนไลน์เป็นช่องทางที่ใช้ในการเผยแพร่หรือค้นหาหรือรับข้อมูลข่าวสาร\",\n",
        "    \"score_2\": \"การใช้สื่อสังคมหรือสื่อออนไลน์หรือสื่อสังคมออนไลน์อย่างไม่ระมัดระวังหรือขาดความรับผิดชอบจะเกิดโทษหรือผลเสียหรือข้อเสียหรือผลกระทบหรือสิ่งไม่ดี\",\n",
        "    \"score_3\": \"ผู้ใช้ต้องรู้ทันหรือรู้เท่าทันสื่อสังคมออนไลน์\",\n",
        "    \"score_4\": \"การใช้สื่อสังคมหรือสื่อออนไลน์หรือสื่อสังคมออนไลน์ด้วยเจตนาแอบแฝงมีผลกระทบต่อความน่าเชื่อถือของข้อมูลข่าวสาร\"\n",
        "}\n",
        "\n",
        "def normalize_text(text):\n",
        "    text = \" \".join(text.replace(\"\\n\", \" \").replace(\"\\r\", \" \").replace(\"\\t\", \" \").split())\n",
        "    return text.replace(\" \", \"\")\n",
        "\n",
        "def find_keywords_list(text, keywords):\n",
        "    found = [kw for kw in keywords if kw.replace(\" \", \"\") in text]\n",
        "    return found\n",
        "\n",
        "def score_group_1(text):\n",
        "    text_norm = normalize_text(text)\n",
        "    media_keywords = [\"สื่อสังคมออนไลน์\", \"สื่อสังคม\", \"สื่อออนไลน์\"]\n",
        "    usage_keywords = [\"เป็นช่องทาง\", \"ช่องทาง\", \"เป็นการแพร่กระจาย\", \"เป็นสื่อ\", \"สามารถ\", \"ทำให้\", \"เป็นการกระจาย\", \"นั้น\"]\n",
        "    last_keywords = [\"แพร่กระจาย\", \"แพร่กระจายข่าวสาร\", \"ค้นหา\", \"รับข้อมูลข่าวสาร\", \"เผยแพร่\", \"ติดต่อสื่อสาร\", \"กระจาย\", \"รับสาร\",\"รับรู้\"]\n",
        "\n",
        "    found_usage = [kw for kw in usage_keywords if kw.replace(\" \", \"\") in text_norm]\n",
        "    found_last = [kw for kw in last_keywords if kw.replace(\" \", \"\") in text_norm]\n",
        "    first_5_words = text.split()[:5]\n",
        "    first_5_text = \"\".join(first_5_words)\n",
        "    found_media_in_first_5 = any(kw in first_5_text for kw in media_keywords)\n",
        "\n",
        "    score = 1 if (found_media_in_first_5 and found_usage and found_last) else 0\n",
        "    return score\n",
        "\n",
        "def score_group_2(text):\n",
        "    text_norm = normalize_text(text)\n",
        "    keypoints_1 = [\"ไม่ระวัง\", \"ไม่ระมัดระวัง\", \"ขาดความรับผิดชอบ\", \"ควรระมัดระวัง\", \"ใช้ในทางที่ไม่ดี\", \"ไม่เหมาะสม\", \"อย่างระมัดระวัง\", \"ไตร่ตรอง\"]\n",
        "    keypoints_2 = [\n",
        "        \"โทษ\", \"ผลเสีย\", \"ข้อเสีย\", \"เกิดผลกระทบ\", \"สิ่งไม่ดี\",\n",
        "        \"เสียหาย\",\n",
        "        \"การเขียนแสดงความเห็นวิพากษ์วิจารณ์ผู้อื่นในทางเสียหาย\",\n",
        "        \"การเขียนแสดงความคิดเห็นวิพากษ์วิจารณ์ผู้อื่นในทางเสียหาย\",\n",
        "        \"ตกเป็นเหยื่อของมิจฉาชีพ\",\n",
        "        \"ตกเป็นเหยื่อมิจฉาชีพ\", \"ตกเป็นเหยื่อทางการตลาด\"\n",
        "    ]\n",
        "    found_1 = find_keywords_list(text_norm, keypoints_1)\n",
        "    found_2 = find_keywords_list(text_norm, keypoints_2)\n",
        "    found_illegal = \"ผิดกฎหมาย\" in text_norm\n",
        "\n",
        "    score = 1 if (found_1 and found_2) or (found_1 and found_illegal and found_2) else 0\n",
        "    return score\n",
        "\n",
        "def score_group_3(text):\n",
        "    text_norm = normalize_text(text)\n",
        "    media_keypoint = [\"สื่อสังคมออนไลน์\", \"สื่อสังคม\", \"สื่อออนไลน์\"]\n",
        "    keypoints = [\"รู้เท่าทัน\", \"รู้ทัน\", \"ผู้ใช้ต้องรู้เท่าทัน\", \"รู้ทันสื่อสังคม\",\n",
        "                 \"รู้เท่าทันสื่อ\", \"รู้ทันสื่อ\", \"สร้างภูมิคุ้มกัน\", \"ไม่ตกเป็นเหยื่อ\", \"แก้ปัญหาการตกเป็นเหยื่อ\"]\n",
        "\n",
        "    found_1 = find_keywords_list(text_norm, media_keypoint)\n",
        "    found_2 = find_keywords_list(text_norm, keypoints)\n",
        "\n",
        "    score = 1 if (found_1 and found_2) else 0\n",
        "    return score\n",
        "\n",
        "def score_group_4(text):\n",
        "    text_norm = normalize_text(text)\n",
        "    media_use_keywords = [\n",
        "        \"ใช้สื่อสังคม\", \"ใช้สื่อออนไลน์\", \"ใช้สื่อสังคมออนไลน์\", \"การใช้สื่อ\"\n",
        "    ]\n",
        "    hidden_intent_keywords = [\"เจตนาแอบแฝง\"]\n",
        "    effect_keywords = [\"ผลกระทบต่อ\", \"ผลกระทบ\"]\n",
        "    credibility_keywords = [\n",
        "        \"ความน่าเชื่อถือของข่าวสาร\", \"ความน่าเชื่อถือของข้อมูลข่าวสาร\", \"ความน่าเชื่อถือของข้อมูล\",\n",
        "        \"มีสติ\", \"ความน่าเชื่อถือ\", \"ความเชื่อถือของข้อมูลข่าวสาร\", \"ข้อมูลข่าวสาร\"\n",
        "    ]\n",
        "    words = text.split()\n",
        "\n",
        "    def find_positions(words, keywords):\n",
        "        positions = []\n",
        "        joined_text = \"\".join(words)\n",
        "        for kw in keywords:\n",
        "            start = 0\n",
        "            while True:\n",
        "                idx = joined_text.find(kw.replace(\" \", \"\"), start)\n",
        "                if idx == -1:\n",
        "                    break\n",
        "                positions.append(len(joined_text[:idx].split()))\n",
        "                start = idx + len(kw.replace(\" \", \"\"))\n",
        "        return positions\n",
        "\n",
        "    media_positions = find_positions(words, media_use_keywords)\n",
        "    hidden_positions = find_positions(words, hidden_intent_keywords)\n",
        "    effect_positions = find_positions(words, effect_keywords)\n",
        "    # ตำแหน่ง media ก่อน hidden หรือ effect (ตามแบบเดิม)\n",
        "    media_before_hidden = any((0 < h - m <= 5) for m in media_positions for h in hidden_positions)\n",
        "    media_before_effect = any((0 < e - m <= 5) for m in media_positions for e in effect_positions)\n",
        "\n",
        "    # ตรวจพบกลุ่ม keyword\n",
        "    found_hidden_intent = find_keywords_list(text_norm, hidden_intent_keywords)\n",
        "    found_effect = find_keywords_list(text_norm, effect_keywords)\n",
        "    found_credibility = find_keywords_list(text_norm, credibility_keywords)\n",
        "\n",
        "    # ต้องเจอทั้ง hidden_intent ผลกระทบ และ credibility ครบทั้ง 3 อย่าง\n",
        "    score = 1 if (found_hidden_intent and found_effect and found_credibility) else 0\n",
        "    return score\n",
        "\n",
        "def evaluate_mind_score(answer_text):\n",
        "    score1 = score_group_1(answer_text)\n",
        "    score2 = score_group_2(answer_text)\n",
        "    score3 = score_group_3(answer_text)\n",
        "    score4 = score_group_4(answer_text)\n",
        "    total_score = score1 + score2 + score3 + score4\n",
        "\n",
        "    result = {\n",
        "        \"ใจความที่ 1\": score1,\n",
        "        \"ใจความที่ 2\": score2,\n",
        "        \"ใจความที่ 3\": score3,\n",
        "        \"ใจความที่ 4\": score4,\n",
        "        \"คะแนนรวมใจความ \": total_score\n",
        "    }\n",
        "\n",
        "    return result\n",
        "\n",
        "# ---- ส่วนให้คะแนนการสะกดคำ ----\n",
        "\n",
        "# โหลด whitelist คำทับศัพท์\n",
        "with open('/content/drive/MyDrive/thai_loanwords_new_update.json', 'r', encoding='utf-8') as f:\n",
        "    loanwords_data = json.load(f)\n",
        "    loanwords_whitelist = set(item['thai_word'] for item in loanwords_data)\n",
        "\n",
        "API_KEY = '33586c7cf5bfa0029887a9831bf94963' # add Apikey\n",
        "API_URL = 'https://api.longdo.com/spell-checker/proof'\n",
        "\n",
        "custom_words = {\"ประเทศไทย\", \"สถาบันการศึกษา\", \"นานาประการ\"}\n",
        "\n",
        "#คำที่สามารถฉีกคำได้\n",
        "splitable_phrases = {\n",
        "    'แม้ว่า', 'ถ้าแม้ว่า', 'แต่ถ้า', 'แต่ทว่า', 'เนื่องจาก', 'ดังนั้น', 'เพราะฉะนั้น','ตกเป็น','เป็นการ',\n",
        "    'ดีแต่', 'หรือไม่', 'ข้อมูลข่าวสาร', 'ทั่วโลก', 'ยังมี', 'ทำให้เกิด', 'เป็นโทษ', 'ไม่มี', 'ข้อควรระวัง', 'การแสดงความคิดเห็น', 'ผิดกฎหมาย', 'แสดงความคิดเห็น'\n",
        "}\n",
        "#คำที่ไม่สามารถฉีกคำได้\n",
        "strict_not_split_words = {\n",
        "    'มากมาย', 'ประเทศไทย', 'ออนไลน์', 'ความคิดเห็น', 'ความน่าเชื่อถือ'\n",
        "}\n",
        "\n",
        "thai_dict = set(w for w in set(thai_words()).union(custom_words) if (' ' not in w) and w.strip())\n",
        "\n",
        "# allowed punctuation (เพิ่ม ' และ \")\n",
        "allowed_punctuations = {'.', ',', '-', '(', ')', '!', '?', '%', '“', '”', '‘', '’', '\"', \"'\", '…', 'ฯ'}\n",
        "\n",
        "# Allow / Forbid list ไม้ยมก (เพิ่มคำที่ใช้บ่อย)\n",
        "allow_list = {'ปี', 'อื่น', 'เล็ก', 'ใหญ่', 'มาก', 'หลาย', 'ช้า', 'เร็ว', 'ชัด', 'ดี', 'ผิด'}\n",
        "forbid_list = {'นา', 'บางคน', 'บางอย่าง', 'บางสิ่ง', 'บางกรณี'}\n",
        "\n",
        "explanations = [\n",
        "    \"1. ตรวจสอบการฉีกคำ\",\n",
        "    \"2. ตรวจสอบคำสะกดผิดด้วย PyThaiNLP (และขอ Longdo ช่วยกรณีสงสัย)\",\n",
        "    \"3. ตรวจสอบการใช้เครื่องหมายที่ไม่อนุญาต\",\n",
        "    \"4. ตรวจสอบการใช้ไม้ยมก (ๆ) ถูกต้องตามบริบทหรือไม่\",\n",
        "    \"5. ตรวจสอบการแยกคำผิด เช่น คำที่ควรติดกัน\"\n",
        "]\n",
        "\n",
        "#------------------------------------------------------------------------#\n",
        "\n",
        "#ตรวจการฉีกคำ\n",
        "def check_linebreak_issue(prev_line_tokens, next_line_tokens, max_words=3):\n",
        "    last_word = prev_line_tokens[-1]\n",
        "    first_word = next_line_tokens[0]\n",
        "    if last_word.endswith('-') or first_word.startswith('-'):\n",
        "        return False, None, None, None\n",
        "    for prev_n in range(1, min(max_words, len(prev_line_tokens)) + 1):\n",
        "        prev_part = ''.join(prev_line_tokens[-prev_n:])\n",
        "        for next_n in range(1, min(max_words, len(next_line_tokens)) + 1):\n",
        "            next_part = ''.join(next_line_tokens[:next_n])\n",
        "            combined = normalize(prev_part + next_part)\n",
        "            if (\n",
        "                (' ' not in combined)\n",
        "                and (combined not in splitable_phrases)\n",
        "                and (\n",
        "                    (combined in strict_not_split_words) or (\n",
        "                        (combined in thai_dict)\n",
        "                        and (len(word_tokenize(combined, engine='newmm')) == 1)\n",
        "                    )\n",
        "                )\n",
        "            ):\n",
        "                return True, prev_part, next_part, combined\n",
        "    return False, None, None, None\n",
        "\n",
        "#วนตรวจทั้งข้อความทีละบรรทัด\n",
        "def analyze_linebreak_issues(text):\n",
        "    lines = text.strip().splitlines()\n",
        "    issues = []\n",
        "    for i in range(len(lines) - 1):\n",
        "        prev_line = lines[i].strip()\n",
        "        next_line = lines[i + 1].strip()\n",
        "        prev_tokens = word_tokenize(prev_line)\n",
        "        next_tokens = word_tokenize(next_line)\n",
        "        if not prev_tokens or not next_tokens:\n",
        "            continue\n",
        "        issue, prev_part, next_part, combined = check_linebreak_issue(prev_tokens, next_tokens)\n",
        "        if issue:\n",
        "            issues.append({\n",
        "                'line_before': prev_line,\n",
        "                'line_after': next_line,\n",
        "                'prev_part': prev_part,\n",
        "                'next_part': next_part,\n",
        "                'combined': combined,\n",
        "                'pos_in_text': (i, len(prev_tokens))\n",
        "            })\n",
        "    return issues\n",
        "\n",
        "#รวมข้อความหรือคำที่ถูกตัดข้ามบรรทัด\n",
        "def merge_linebreak_words(text, linebreak_issues):\n",
        "    lines = text.splitlines()\n",
        "    for issue in reversed(linebreak_issues):\n",
        "        i, _ = issue['pos_in_text']\n",
        "        lines[i] = lines[i].rstrip() + issue['combined'] + lines[i+1].lstrip()[len(issue['next_part']):]\n",
        "        lines.pop(i+1)\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "#ตรวจการสสะกดคำ pythainlp + longdo\n",
        "def pythainlp_spellcheck(tokens, pos_tags, dict_words=None, ignore_words=None):\n",
        "    if dict_words is None:\n",
        "        dict_words = thai_dict\n",
        "    if ignore_words is None:\n",
        "        ignore_words = set()\n",
        "    misspelled = []\n",
        "    for i, w in enumerate(tokens):\n",
        "        if not w.strip() or w in dict_words or w in ignore_words or len(w) == 1 or 'ๆ' in w:\n",
        "            continue\n",
        "        misspelled.append({\n",
        "            'word': w,\n",
        "            'pos': pos_tags[i][1] if i < len(pos_tags) else None,\n",
        "            'index': i\n",
        "        })\n",
        "    return misspelled\n",
        "\n",
        "def longdo_spellcheck_batch(words):\n",
        "    results = {}\n",
        "    if not words:\n",
        "        return results\n",
        "    try:\n",
        "        headers = {'Content-Type': 'application/json'}\n",
        "        payload = {\"key\": API_KEY, \"text\": \"\\n\".join(words)}\n",
        "        response = requests.post(API_URL, headers=headers, json=payload, timeout=6)\n",
        "        if response.status_code == 200:\n",
        "            result = response.json()\n",
        "            for e in result.get(\"result\", []):\n",
        "                if e.get(\"suggestions\"):\n",
        "                    results[e[\"word\"]] = e[\"suggestions\"]\n",
        "    except Exception as e:\n",
        "        print(f\"Exception calling longdo: {e}\")\n",
        "    return results\n",
        "\n",
        "#ตรวจการสะกดคำของคำทับศัพท์\n",
        "def check_loanword_spelling(tokens,loanwords_whitelist):\n",
        "    mistakes = []\n",
        "    for tok in tokens:\n",
        "        # Find close matches with a lower cutoff for loanwords\n",
        "        matches = difflib.get_close_matches(tok, list(loanwords_whitelist), n=1, cutoff=0.7) # Lowered cutoff\n",
        "        if matches and tok not in loanwords_whitelist:\n",
        "            mistakes.append({'found': tok, 'should_be': matches[0]})\n",
        "    return mistakes\n",
        "\n",
        "#ตรวจการใช้เครื่องหมายที่ไม่อนุญาต\n",
        "def find_unallowed_punctuations(text):\n",
        "    pattern = f\"[^{''.join(re.escape(p) for p in allowed_punctuations)}a-zA-Z0-9ก-๙\\\\s]\"\n",
        "    return set(re.findall(pattern, text))\n",
        "\n",
        "#ใช้แยกไม้ยมกออกจากคำที่ติดกัน\n",
        "def separate_maiyamok(text):\n",
        "    return re.sub(r'(\\S+?)ๆ', r'\\1 ๆ', text)\n",
        "#ตรวจการใช้ไม้ยมก\n",
        "def analyze_maiyamok(tokens, pos_tags):\n",
        "    results = []\n",
        "    found_invalid = False\n",
        "    VALID_POS = {'NCMN', 'NNP', 'VACT', 'VNIR', 'CLFV', 'ADVN', 'ADVI', 'ADVP', 'PRP', 'ADV'}\n",
        "    for i, token in enumerate(tokens):\n",
        "        if token == 'ๆ':\n",
        "            prev_idx = i - 1\n",
        "            prev_word = tokens[prev_idx] if prev_idx >= 0 else None\n",
        "            prev_tag = pos_tags[prev_idx][1] if prev_idx >= 0 else None\n",
        "            if prev_word is None or prev_word == 'ๆ':\n",
        "                verdict = \"❌ ไม้ยมกไม่ควรขึ้นต้นประโยค/คำ\"\n",
        "            elif prev_word in forbid_list:\n",
        "                verdict = '❌ ไม่ควรใช้ไม้ยมกกับคำนี้'\n",
        "            elif (prev_tag in VALID_POS) or (prev_word in allow_list):\n",
        "                verdict = '✅ ถูกต้อง (ใช้ไม้ยมกซ้ำคำได้)'\n",
        "            else:\n",
        "                verdict = '❌ ไม่ควรใช้ไม้ยมok นอกจากกับคำนาม/กริยา/วิเศษณ์'\n",
        "            context = tokens[max(0, i-2):min(len(tokens), i+3)]\n",
        "            results.append({\n",
        "                'คำก่อนไม้ยมก': prev_word or '',\n",
        "                'POS คำก่อน': prev_tag or '',\n",
        "                'บริบท': ' '.join(context),\n",
        "                'สถานะ': verdict\n",
        "            })\n",
        "            if verdict.startswith('❌'):\n",
        "                found_invalid = True\n",
        "    return results, found_invalid\n",
        "\n",
        "#ตรวจการแยกคำ\n",
        "def detect_split_errors(tokens, custom_words=None):\n",
        "    check_dict = set(thai_words()).union(custom_words or [])\n",
        "    check_dict = {w for w in check_dict if (' ' not in w) and w.strip()}\n",
        "    errors = []\n",
        "    for i in range(len(tokens) - 1):\n",
        "        combined = tokens[i] + tokens[i + 1]\n",
        "        if (' ' not in combined) and (combined in check_dict) and (combined not in splitable_phrases):\n",
        "            errors.append({\n",
        "                \"split_pair\": (tokens[i], tokens[i+1]),\n",
        "                \"suggested\": combined\n",
        "            })\n",
        "    return errors\n",
        "\n",
        "def evaluate_text(text):\n",
        "    # วิเคราะห์\n",
        "    linebreak_issues = analyze_linebreak_issues(text)\n",
        "    corrected_text = merge_linebreak_words(text, linebreak_issues)\n",
        "    tokens = word_tokenize(corrected_text, engine='newmm', keep_whitespace=False)\n",
        "    pos_tags = pos_tag(tokens, corpus='orchid')\n",
        "\n",
        "    # ตรวจคำทับศัพท์\n",
        "    loanword_spell_errors = check_loanword_spelling(tokens, loanwords_whitelist)\n",
        "\n",
        "    # ตรวจสะกด\n",
        "    pythai_errors = pythainlp_spellcheck(tokens, pos_tags, dict_words=thai_dict, ignore_words=custom_words)\n",
        "    wrong_words = [e['word'] for e in pythai_errors]\n",
        "    longdo_results = longdo_spellcheck_batch(wrong_words)\n",
        "    spelling_errors_legit = [\n",
        "        {**e, 'suggestions': longdo_results.get(e['word'], [])}\n",
        "        for e in pythai_errors if e['word'] in longdo_results\n",
        "    ]\n",
        "\n",
        "    # อื่น ๆ\n",
        "    punct_errors = find_unallowed_punctuations(text)\n",
        "    maiyamok_results, has_wrong_maiyamok = analyze_maiyamok(tokens, pos_tags)\n",
        "    split_errors = detect_split_errors(tokens, custom_words=custom_words)\n",
        "\n",
        "    # ==== นับจำนวนข้อผิดพลาดแต่ละประเภท ====\n",
        "    error_counts = {\n",
        "        \"spelling\": len(spelling_errors_legit) + len(loanword_spell_errors),\n",
        "        \"linebreak\": len(linebreak_issues),\n",
        "        \"split\": len(split_errors),\n",
        "        \"punct\": len(punct_errors),\n",
        "        \"maiyamok\": sum(1 for r in maiyamok_results if r['สถานะ'].startswith('❌'))\n",
        "    }\n",
        "    n_issue_types = sum(1 for c in error_counts.values() if c > 0)\n",
        "    multi_in_single_type = any(c >= 2 for c in error_counts.values())\n",
        "\n",
        "    # ==== สร้าง reasons ====\n",
        "    reasons = []\n",
        "    if error_counts[\"linebreak\"]:\n",
        "        details = [f\"{issue['prev_part']} + {issue['next_part']} → {issue['combined']}\" for issue in linebreak_issues]\n",
        "        reasons.append(\"พบการฉีกคำข้ามบรรทัด: \" + \"; \".join(details))\n",
        "    if error_counts[\"split\"]:\n",
        "        details = [f\"{e['split_pair'][0]} + {e['split_pair'][1]} → {e['suggested']}\" for e in split_errors]\n",
        "        reasons.append(\"พบการแยกคำผิด: \" + \"; \".join(details))\n",
        "    if error_counts[\"spelling\"]:\n",
        "        error_words = [e['word'] for e in spelling_errors_legit]\n",
        "        error_desc = [f\"{e['found']} (ควรเป็น {e['should_be']})\" for e in loanword_spell_errors]\n",
        "        reasons.append(f\"ตรวจเจอคำสะกดผิดหรือทับศัพท์ผิด: {', '.join(error_words + error_desc)}\")\n",
        "    if error_counts[\"punct\"]:\n",
        "        reasons.append(f\"ใช้เครื่องหมายที่ไม่อนุญาต: {', '.join(punct_errors)}\")\n",
        "    if error_counts[\"maiyamok\"]:\n",
        "        wrong_desc = [x for x in maiyamok_results if x['สถานะ'].startswith('❌')]\n",
        "        texts = [f\"{x['คำก่อนไม้ยมก']}: {x['สถานะ']}\" for x in wrong_desc]\n",
        "        reasons.append(\"ใช้ไม้ยมกผิด: \" + '; '.join(texts))\n",
        "    if not reasons:\n",
        "        reasons.append(\"ไม่มีปัญหา\")\n",
        "\n",
        "    # ==== เกณฑ์การให้คะแนน ====\n",
        "    if sum(error_counts.values()) == 0:\n",
        "        score = 1.0\n",
        "    elif n_issue_types == 1 and multi_in_single_type:\n",
        "        score = 0.0\n",
        "    elif n_issue_types == 1:\n",
        "        score = 0.5\n",
        "    else:\n",
        "        score = 0.0\n",
        "\n",
        "    return {\n",
        "        'linebreak_issues': linebreak_issues,\n",
        "        'spelling_errors': spelling_errors_legit,\n",
        "        'loanword_spell_errors': loanword_spell_errors,\n",
        "        'punctuation_errors': list(punct_errors),\n",
        "        'maiyamok_results': maiyamok_results,\n",
        "        'split_errors': split_errors,\n",
        "        'reasons': reasons,\n",
        "         'score': score\n",
        "       # 'explanations': explanations\n",
        "    }\n",
        "\n",
        "def evaluate_single_answer(answer_text):\n",
        "\n",
        "    # --- ตรวจความใกล้เคียงด้วย cosine similarity ---\n",
        "    student_emb = model.encode(answer_text, convert_to_tensor=True)\n",
        "    core_embs = model.encode(list(core_sentences.values()), convert_to_tensor=True)\n",
        "    cosine_scores = util.cos_sim(student_emb, core_embs)[0]\n",
        "    best_score = cosine_scores.max().item()\n",
        "\n",
        "    # ถ้า similarity < 0.6 → ไม่ให้คิดคะแนนใจความสำคัญเลย\n",
        "    if best_score < 0.6:\n",
        "        result = {\n",
        "            \"cosine_similarity\": best_score,\n",
        "            \"คะแนนใจความสำคัญ\": {\"ใจความที่ 1\": 0, \"ใจความที่ 2\": 0,\n",
        "                                \"ใจความที่ 3\": 0, \"ใจความที่ 4\": 0,\n",
        "                                \"คะแนนรวมใจความ \": 0},\n",
        "            \"คะแนนการสะกดคำ\": 0.0,\n",
        "            \"เงื่อนไขที่ผิด\": [\"Cosine similarity < 0.6 ไม่ตรวจใจความสำคัญ\"],\n",
        "            \"คะแนนรวมทั้งหมด\": 0.0\n",
        "        }\n",
        "        return json.dumps(result, ensure_ascii=False, indent=2)\n",
        "\n",
        "\n",
        "    # คะแนนใจความ\n",
        "    # --- ถ้า similarity ≥ 0.6 → คำนวณใจความตามปกติ ---\n",
        "    mind_score = evaluate_mind_score(answer_text)\n",
        "    mind_total = mind_score[\"คะแนนรวมใจความ \"]\n",
        "\n",
        "    if mind_total == 0:\n",
        "        spelling_score = 0.0\n",
        "        combined_score = 0.0\n",
        "        spelling_reason = [\"ไม่ได้คะแนนใจความสำคัญ (ใจความ = 0) จึงไม่ตรวจการสะกดคำ\"]\n",
        "    else:\n",
        "        # คะแนนการสะกดคำ\n",
        "        res = evaluate_text(str(answer_text))\n",
        "        spelling_score = res[\"score\"]   # ได้เป็น 0.0 / 0.5 / 1.0\n",
        "\n",
        "        # รวมคะแนน (ถ่วงน้ำหนักหรือบวกตามที่ต้องการ)\n",
        "        combined_score = mind_total + spelling_score\n",
        "        spelling_reason = res[\"reasons\"]\n",
        "\n",
        "    result = {\n",
        "        \"cosine_similarity\": best_score,\n",
        "        \"คะแนนใจความสำคัญ (4 คะแนน)\": mind_score,\n",
        "        \"คะแนนการสะกดคำ (1 คะแนน)\": spelling_score,\n",
        "        \"เงื่อนไขที่ผิด\": spelling_reason,\n",
        "        \"คะแนนรวมทั้งหมด\": combined_score\n",
        "    }\n",
        "    return json.dumps(result, ensure_ascii=False, indent=2)\n",
        "\n",
        "# --- ตัวอย่างการใช้งาน ---\n",
        "answer = \"\"\"สื่อสังคม หรือที่คนทั่วไปเรียกว่า สื่อออนไลน์ นั้น เป็นสื่อหรือช่องทางที่แพร่กระจาย\n",
        "ข้อมูลได้อย่างรวดเร็วไปยังผู้คนที่อยู่ทั่วโลกที่สัญญาณโทรศัพท์เข้าถึง เช่น นำเสนอ\n",
        "ข้อ\n",
        "ดี ของสินค้าชั้นนำ สินค้าพื้นเมืองให้เข้าถึงผู้ซื้อได้ การนำเสนอข้อเท็จจริงของข่าวสาร\n",
        "การเผยแพร่งานเขียนคุณภาพบนออนไลน์แทนสำนักพิมพ์ จึงกล่าวได้ว่า เราสามารถใช้\n",
        "สื่อสังคมออนไลน์ รับข้อมูลข่าวสารได้เป็นอย่างดี\"\"\"\n",
        "\n",
        "print(\"ผลลัพธ์\")\n",
        "print(evaluate_single_answer(answer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GuBEcEQc7ki",
        "outputId": "8d7ba798-55f6-4345-ea75-70bb9f6ec2eb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ผลลัพธ์\n",
            "{\n",
            "  \"cosine_similarity\": 0.9129053354263306,\n",
            "  \"คะแนนใจความสำคัญ (4 คะแนน)\": {\n",
            "    \"ใจความที่ 1\": 1,\n",
            "    \"ใจความที่ 2\": 0,\n",
            "    \"ใจความที่ 3\": 0,\n",
            "    \"ใจความที่ 4\": 0,\n",
            "    \"คะแนนรวมใจความ \": 1\n",
            "  },\n",
            "  \"คะแนนการสะกดคำ (1 คะแนน)\": 0.0,\n",
            "  \"เงื่อนไขที่ผิด\": [\n",
            "    \"พบการฉีกคำข้ามบรรทัด: ข้อ + ดี → ข้อดี\",\n",
            "    \"พบการแยกคำผิด: ได้ + การ → ได้การ\"\n",
            "  ],\n",
            "  \"คะแนนรวมทั้งหมด\": 1.0\n",
            "}\n"
          ]
        }
      ]
    }
  ]
}